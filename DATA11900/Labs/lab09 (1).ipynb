{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8: Regularization and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.linear_model as lm\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's lab covers:\n",
    "\n",
    "- How to use regularization to avoid overfitting\n",
    "- How to use cross-validation to find the amount of regularization that produces a model with the least error for new data\n",
    "\n",
    "## Dammed Data\n",
    "\n",
    "We've put our data into two files: `train.csv` and `test.csv` which contain\n",
    "the training and test data, respectively. You are not allowed to train\n",
    "on the test data.\n",
    "\n",
    "The y values in the training data correspond to the amount of water that flows out of the dam on a particular day.  There is only 1 feature: the increase in water level for the dam's reservoir on that day, which we'll call x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y\n",
      "-15.93675813378541,2.1343105067296686\n",
      "-29.152979217238133,1.1732566787564553\n",
      "36.18954862666253,34.35910918053895\n",
      "37.49218733199513,36.83795516371235\n",
      "-48.058829452570066,2.808965074479856\n",
      "-8.941457938049755,2.121072476666392\n",
      "15.307792889226079,14.710268306562307\n",
      "-34.70626581132249,2.614184386432259\n",
      "1.3891543686358903,3.7401716656949393\n"
     ]
    }
   ],
   "source": [
    "!head train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33, 1), (33,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "X = data[['X']].values\n",
    "y = data['y'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Water flow out of dam (y)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.plot(X[:, 0], y, '.')\n",
    "plt.xlabel('Change in water level (X)')\n",
    "plt.ylabel('Water flow out of dam (y)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_curve(curve_x, curve_y):\n",
    "    plt.plot(X[:, 0], y, '.')\n",
    "    plt.plot(curve_x, curve_y, '-')\n",
    "    plt.ylim(-20, 60)\n",
    "    plt.xlabel('Change in water level (X)')\n",
    "    plt.ylabel('Water flow out of dam (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** As a warmup, let's fit a line to this data using `sklearn`.\n",
    "We've imported [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) as `lm`, so you can use that instead of\n",
    "typing out the whole module name. Running the cell should show the data\n",
    "with your best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-432ee8ba9103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit your classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlinear_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Predict a bunch of points to draw best fit line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "linear_clf = ...\n",
    "\n",
    "# Fit your classifier\n",
    "linear_clf.fit(X, y)\n",
    "\n",
    "# Predict a bunch of points to draw best fit line\n",
    "all_x = np.linspace(-55, 55, 200).reshape(-1, 1)\n",
    "line = linear_clf.predict(all_x)\n",
    "\n",
    "plot_data_and_curve(all_x, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "linear_clf = lm.LinearRegression()\n",
    "# Fit your classifier\n",
    "linear_clf.fit(X, y)\n",
    "\n",
    "all_x = np.linspace(-55, 55, 200).reshape(-1, 1)\n",
    "line = linear_clf.predict(all_x)\n",
    "\n",
    "plot_data_and_curve(all_x, line)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** If you had to guess, which has a larger effect on error for this dataset: bias or variance?\n",
    "Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<b>SOLUTION:</b> Bias. Our data are curved up, but our line cannot model that curve, so bias is high. Variance is low because the complexity of our model is low relative to the size of the training set.  That is, if we drew a new dataset from the population of days, we would probably get a similar line.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Let's now add some complexity to our model by adding polynomial features.\n",
    "\n",
    "Reference the `sklearn` docs on the `PolynomialFeatures` class. You should use this class to add polynomial features to `X` up to degree 8 using the `fit_transform` method.\n",
    "\n",
    "The final `X_poly` data matrix should have shape `(33, 9)`. Think about and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_poly = ...\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "X_poly = PolynomialFeatures(degree=8).fit_transform(X)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Now, fit a linear regression to the data, using polynomial features.\n",
    "\n",
    "Then, follow the code in Question 1 to make predictions for the values in `all_x`. You'll have to add polynomial features to `all_x` in order to make predictions.\n",
    "\n",
    "Then, running this cell should plot the best fit curve using a degree 8 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_clf = ...\n",
    "\n",
    "# Fit your classifier\n",
    "...\n",
    "\n",
    "# Set curve to your model's predictions on all_x\n",
    "curve = ...\n",
    "\n",
    "plot_data_and_curve(all_x, curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "poly_clf = lm.LinearRegression()\n",
    "\n",
    "poly_clf.fit(X_poly, y)\n",
    "\n",
    "curve = poly_clf.predict(PolynomialFeatures(degree=8).fit_transform(all_x))\n",
    "\n",
    "plot_data_and_curve(all_x, curve)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Think about and discuss what you notice in the model's predictions.\n",
    "\n",
    "Now, compute the mean squared training error for both the best fit line and polynomial. Again, you'll have to transform the training data for the polynomial regression before you can make predictions.\n",
    "\n",
    "You should get training errors of around 52.8 and 5.23 for line and polynomial models, respectively. Why does the polynomial model get a lower training error than the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_y, actual_y):\n",
    "    return np.mean((predicted_y - actual_y) ** 2)\n",
    "\n",
    "line_training_error = ...\n",
    "poly_training_error = ...\n",
    "\n",
    "line_training_error, poly_training_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "line_training_error = mse(linear_clf.predict(X), y)\n",
    "poly_training_error = mse(poly_clf.predict(PolynomialFeatures(degree=8).fit_transform(X)), y)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** It's annoying to have to transform the data every time we want to use polynomial features. We can use a [`Pipeline`](http://scikit-learn.org/stable/modules/pipeline.html#pipeline) to let us do both transformation and regression in one step.\n",
    "\n",
    "Read the docs for [`make_pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline) and create a pipeline for polynomial regression called `poly_pipeline`. Then, fit it on `X` and `y` and compute the training error as in Question 5. The training errors should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "poly_pipeline = ...\n",
    "\n",
    "# Fit the pipeline on X and y\n",
    "...\n",
    "\n",
    "# Compute the training error\n",
    "pipeline_training_error = ...\n",
    "pipeline_training_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "poly_pipeline = make_pipeline(PolynomialFeatures(degree=8), lm.LinearRegression())\n",
    "\n",
    "poly_pipeline.fit(X, y)\n",
    "\n",
    "pipeline_training_error = mse(poly_pipeline.predict(X), y)\n",
    "pipeline_training_error\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! With pipelines, we can combine any number of transformations and treat the whole thing as a single classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** Now, we know that a low training error doesn't necessarily mean your model is good. So, we'll hold out some points from the training data for a *validation set*. We'll use these held-out points to choose the best model.\n",
    "\n",
    "Use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split out one third of the training data for validation. Call the resulting datasets `X_train`, `X_valid`, `y_train`, `y_valid`.\n",
    "\n",
    "`X_train` should have shape `(22, 1)`. `X_valid` should have shape `(11, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "...\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33)\n",
    "X_train.shape, X_valid.shape\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Now, set `X_train_line_error`, `X_valid_line_error`,\n",
    "`X_train_poly_error`, `X_valid_poly_error` to the training and validation\n",
    "errors for both linear and polynomial regression.\n",
    "\n",
    "You'll have to call `.fit` on your classifiers/pipelines again since we're using\n",
    "`X_train` and `y_train` instead of `X` and `y`.\n",
    "\n",
    "You should see that the validation error for the polynomial fit is significantly\n",
    "higher than the linear fit (152.6 vs 115.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear classifier\n",
    "...\n",
    "\n",
    "# Fit the polynomial pipeline\n",
    "...\n",
    "\n",
    "X_train_line_error = ...\n",
    "X_valid_line_error = ...\n",
    "\n",
    "X_train_poly_error = ...\n",
    "X_valid_poly_error = ...\n",
    "\n",
    "X_train_line_error, X_valid_line_error, X_train_poly_error, X_valid_poly_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "# Fit the linear classifier\n",
    "linear_clf.fit(X_train, y_train)\n",
    "\n",
    "poly_pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_train_line_error = mse(linear_clf.predict(X_train), y_train)\n",
    "X_valid_line_error = mse(linear_clf.predict(X_valid), y_valid)\n",
    "\n",
    "X_train_poly_error = mse(poly_pipeline.predict(X_train), y_train)\n",
    "X_valid_poly_error = mse(poly_pipeline.predict(X_valid), y_valid)\n",
    "\n",
    "X_train_line_error, X_valid_line_error, X_train_poly_error, X_valid_poly_error\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** Our 8 degree polynomial is overfitting our data.\n",
    "To reduce overfitting, we can use **regularization**.\n",
    "\n",
    "The usual cost function for linear regression is:\n",
    "\n",
    "$$J(\\theta) = (Y - X \\theta)^T (Y - X \\theta)$$\n",
    "\n",
    "Edit the cell below to show the cost function of linear regressions with L2 regularization. Use\n",
    "$\\lambda $ as your regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = (Y - X \\theta)^T (Y - X \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now,** explain why this cost function helps reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "    <b>SOLUTION:</b> Adding regularization effectively restricts the set of possible polynomials we're allowed to use to fit the data.  In particular, polynomials with large coefficients, which can be used to fit data very precisely, are discouraged.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** L2 regularization for linear regression is also known as\n",
    "*Ridge* regression. Create a pipeline called `ridge_pipeline` that again\n",
    "creates polynomial features with degree 8 and then uses the `Ridge` sklearn \n",
    "classifier.\n",
    "\n",
    "The `alpha` argument is the same as our $\\lambda$. Leave it as the default (1.0). You should set `normalize=True` to normalize your data before fitting. Why do we have to do this?\n",
    "\n",
    "Then, fit your pipeline on the data. The cell will then plot the curve of your\n",
    "regularized classifier. You should notice the curve is significantly\n",
    "smoother.\n",
    "\n",
    "Then, fiddle around with the alpha value. What do you notice as you\n",
    "increase alpha? Decrease alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline = ...\n",
    "\n",
    "# Fit your classifier\n",
    "...\n",
    "\n",
    "# Set curve to your model's predictions on all_x\n",
    "ridge_curve = ...\n",
    "\n",
    "plot_data_and_curve(all_x, ridge_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "ridge_pipeline = make_pipeline(PolynomialFeatures(degree=8), lm.Ridge(normalize=True, alpha=1.)) #SOLUTION\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "ridge_curve = ridge_pipeline.predict(all_x)\n",
    "\n",
    "plot_data_and_curve(all_x, ridge_curve)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11:** Compute the training and validation error for the `ridge_pipeline`.\n",
    "\n",
    "How do the errors compare to the errors for the unregularized model? Why did each one go up/down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_error = ...\n",
    "ridge_valid_error = ...\n",
    "\n",
    "ridge_train_error, ridge_valid_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "ridge_train_error = mse(ridge_pipeline.predict(X_train), y_train) \n",
    "ridge_valid_error = mse(ridge_pipeline.predict(X_valid), y_valid)\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12:** Now we want to know: how do we choose the best alpha value?\n",
    "\n",
    "This is where we use our validation set. We can try out a bunch of alphas and pick the one that gives us the least error on the validation set. Why can't we use the one that gives us the least error on the training set? The test set?\n",
    "\n",
    "For each alpha in the given `alphas` list, fit a Ridge regression model to the training set and check its accuracy on the validation set.\n",
    "\n",
    "Finally, set `best_alpha` to the best value. You should get a best alpha of 0.01 with a validation error of 15.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "for_assignment_type": "student",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# Your code to find the best alpha\n",
    "...\n",
    "\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "def compute_error(alpha):\n",
    "    pline = make_pipeline(PolynomialFeatures(degree=8),\n",
    "                          lm.Ridge(normalize=True, alpha=alpha))\n",
    "    pline.fit(X_train, y_train)\n",
    "    return mse(pline.predict(X_valid), y_valid)\n",
    "\n",
    "errors = [compute_error(alpha) for alpha in alphas]\n",
    "best_alpha_idx = np.argmin(errors)\n",
    "best_alpha, best_error = alphas[best_alpha_idx], errors[best_alpha_idx]\n",
    "\n",
    "best_alpha, best_error\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13:** Now, set `best_pipeline` to the pipeline with the degree 8 polynomial transform and the ridge regression model with the best value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = ...\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_curve = best_pipeline.predict(all_x)\n",
    "\n",
    "plot_data_and_curve(all_x, best_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><button>Click here to reveal the answer!</button></summary>\n",
    "<pre>\n",
    "best_pipeline = make_pipeline(PolynomialFeatures(degree=8), lm.Ridge(normalize=True, alpha=best_alpha))\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to find the test error of your simple linear model, your polynomial model, and your regularized polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "X_test = data[['X']].as_matrix()\n",
    "y_test = data['y'].as_matrix()\n",
    "\n",
    "line_test_error = mse(linear_clf.predict(X_test), y_test)\n",
    "poly_test_error = mse(poly_pipeline.predict(X_test), y_test)\n",
    "best_test_error = mse(best_pipeline.predict(X_test), y_test)\n",
    "\n",
    "line_test_error, poly_test_error, best_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You've use regularization and cross-validation to fit an accurate polynomial model to the dataset.\n",
    "\n",
    "In the future, you'd probably want to use something like [`RidgeCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV) to automatically perform cross-validation, but it's instructive to do it yourself at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement\n",
    "\n",
    "This lab has been taken from the [Data 100 course at UC Berkeley](http://www.ds100.org/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
